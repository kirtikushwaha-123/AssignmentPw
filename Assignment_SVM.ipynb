{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM)\n",
        "\n",
        "H1: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "    A supervised learning algorithm used for classification and regression that finds the optimal hyperplane separating data into classes with the maximum margin.\n",
        "\n",
        "H2: What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "    Hard Margin: No misclassification allowed; assumes data is perfectly separable. Soft\n",
        "    Margin: Allows some misclassifications using penalty parameter (C) for noisy data.\n",
        "\n",
        "H3: What is the mathematical intuition behind SVM?\n",
        "\n",
        "    SVM maximizes the margin between classes. Minimize (1/2)||w||² subject to y■(w·x■ + b)\n",
        "    ≥ 1. Margin = 2/||w||.\n",
        "\n",
        "H4: What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "    They convert the constrained optimization problem into a dual form, making it easier to solve and helping identify support vectors.\n",
        "\n",
        "H5: What are Support Vectors in SVM?\n",
        "\n",
        "    Data points closest to the hyperplane; they define and affect the position of the decision boundary.\n",
        "\n",
        "H6: What is a Support Vector Classifier (SVC)?\n",
        "    A classifier using the SVM algorithm to separate data into classes by finding the optimal hyperplane.\n",
        "\n",
        "H7: What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "    The regression version of SVM; fits data within a margin of tolerance (ε).\n",
        "\n",
        "H8: What is the Kernel Trick in SVM?\n",
        "\n",
        "    A method to handle non-linear data by mapping it into higher dimensions using a kernel function.\n",
        "\n",
        "H9: Compare Linear, Polynomial, and RBF Kernels.\n",
        "    Linear: K(x,x') = x·x' (linearly separable data). Polynomial: K(x,x') = (x·x' + c)^d (polynomial relationships). RBF: K(x,x') = e^(−γ||x−x'||²) (complex, non-linear data).\n",
        "\n",
        "H10: What is the effect of the C parameter in SVM?\n",
        "\n",
        "    C controls trade-off between margin width and misclassification. High C: less tolerance for errors. Low C: wider margin, more tolerance.\n",
        "\n",
        "H11: What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "    Gamma defines influence of a single training example. High γ: nearby points influence more (complex boundary). Low γ: smoother decision boundary."
      ],
      "metadata": {
        "id": "xwefn7RAKL3E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3XlIEYvKKzs"
      },
      "outputs": [],
      "source": []
    }
  ]
}